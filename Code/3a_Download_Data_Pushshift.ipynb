{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Summary:\n",
    "\n",
    "This file downloads data from Reddit using the pushshift api.\n",
    "\n",
    "After my initial analysis in notebooks 1 and 2, I decided I wanted a lot of data so I could better analyze the urls.  So I downloaded up to 100 posts per day (the max allowed), and collected data for the last 8 years in each subreddit.  \n",
    "\n",
    "Because the computer sometimes timed out due to inactivity, I ended up having to collect the data in chunks.  However, because I had a counter on the days, it was easy to restart where I left off with some overlap.  In notebook 3b, I accounted for the overlapping days in my data.\n",
    "\n",
    "After downloading the posts, I saved the information as separate files, so they could be cleaned up and combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'subreddit': 'Republican',\n",
    "    'before':'1d',\n",
    "    'after':'2d',\n",
    "    'size': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data for days in range 1 to 365 from today\n",
    "**all were successful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "for i in range(1,365):\n",
    "    days.append(str(i) + 'd')\n",
    "\n",
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reps = []\n",
    "list_of_dems = []\n",
    "\n",
    "for i in range(len(days)-1):\n",
    "\n",
    "    # For Republican subreddit\n",
    "    params = {\n",
    "        'subreddit': 'Republican',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params)\n",
    "    print(res.status_code)\n",
    "\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    df_posts = pd.DataFrame(posts)\n",
    "    print(len(posts))\n",
    "    list_of_reps.append(df_posts)\n",
    "    \n",
    "    \n",
    "    # For democrats subreddit\n",
    "    params2 = {\n",
    "        'subreddit': 'democrats',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res2 = requests.get(url, params2)\n",
    "    print(res2.status_code)\n",
    "\n",
    "    data2 = res2.json()\n",
    "    posts2 = data2['data']\n",
    "    df_posts2 = pd.DataFrame(posts2)\n",
    "    print(len(posts2))\n",
    "    list_of_dems.append(df_posts2)\n",
    "    \n",
    "    \n",
    "    #time between requests\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(list_of_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(list_of_dems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data for days in range 365 to 730 from today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "for i in range(365,730):\n",
    "    days.append(str(i) + 'd')\n",
    "\n",
    "list_of_reps2 = []\n",
    "list_of_dems2 = []\n",
    "\n",
    "for i in range(len(days)-1):\n",
    "\n",
    "    print(i)\n",
    "    # For Republican subreddit\n",
    "    params = {\n",
    "        'subreddit': 'Republican',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params)\n",
    "    print(res.status_code)\n",
    "\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    df_posts = pd.DataFrame(posts)\n",
    "    print(len(posts))\n",
    "    list_of_reps2.append(df_posts)\n",
    "    \n",
    "    \n",
    "    # For democrats subreddit\n",
    "    params2 = {\n",
    "        'subreddit': 'democrats',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res2 = requests.get(url, params2)\n",
    "    print(res2.status_code)\n",
    "\n",
    "    data2 = res2.json()\n",
    "    posts2 = data2['data']\n",
    "    df_posts2 = pd.DataFrame(posts2)\n",
    "    print(len(posts2))\n",
    "    list_of_dems2.append(df_posts2)\n",
    "    \n",
    "    \n",
    "    #time between requests\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(list_of_dems2)['created_utc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(list_of_dems2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.fromtimestamp(1571584267, timezone.utc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## because of the errors I'm getting, I add a day counter, so it's easy to keep track of the days that successfully download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data for days in range 640 to 2190 from today\n",
    "**downloads up to day 1677 were successful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "for i in range(640,2190):\n",
    "    days.append(str(i) + 'd')\n",
    "\n",
    "list_of_reps3 = []\n",
    "list_of_dems3 = []\n",
    "\n",
    "for i in range(len(days)-1):\n",
    "\n",
    "    print(f'day is {days[i]}')\n",
    "    # For Republican subreddit\n",
    "    params = {\n",
    "        'subreddit': 'Republican',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params)\n",
    "    print(res.status_code)\n",
    "\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    df_posts = pd.DataFrame(posts)\n",
    "    print(len(posts))\n",
    "    list_of_reps3.append(df_posts)\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # For democrats subreddit\n",
    "    params2 = {\n",
    "        'subreddit': 'democrats',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res2 = requests.get(url, params2)\n",
    "    print(res2.status_code)\n",
    "\n",
    "    data2 = res2.json()\n",
    "    posts2 = data2['data']\n",
    "    df_posts2 = pd.DataFrame(posts2)\n",
    "    print(len(posts2))\n",
    "    list_of_dems3.append(df_posts2)\n",
    "    \n",
    "    \n",
    "    #time between requests\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data for days in range 1665 to 2190 from today\n",
    "**downloads up to day 2182 were successful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "for i in range(1665,2190):\n",
    "    days.append(str(i) + 'd')\n",
    "\n",
    "list_of_reps4 = []\n",
    "list_of_dems4 = []\n",
    "\n",
    "for i in range(len(days)-1):\n",
    "\n",
    "    print(f'day is {days[i]}')\n",
    "    # For Republican subreddit\n",
    "    params = {\n",
    "        'subreddit': 'Republican',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params)\n",
    "    print(res.status_code)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        df_posts = pd.DataFrame(posts)\n",
    "        print(len(posts))\n",
    "        list_of_reps4.append(df_posts)\n",
    "    \n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # For democrats subreddit\n",
    "    params2 = {\n",
    "        'subreddit': 'democrats',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res2 = requests.get(url, params2)\n",
    "    print(res2.status_code)\n",
    "\n",
    "    if res2.status_code == 200:\n",
    "        data2 = res2.json()\n",
    "        posts2 = data2['data']\n",
    "        df_posts2 = pd.DataFrame(posts2)\n",
    "        print(len(posts2))\n",
    "        list_of_dems4.append(df_posts2)\n",
    "    \n",
    "    \n",
    "    #time between requests\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data for days in range 2180 to 3000 from today\n",
    "**downloads up to day 2246 were successful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "for i in range(2180,3000):\n",
    "    days.append(str(i) + 'd')\n",
    "\n",
    "list_of_reps5 = []\n",
    "list_of_dems5 = []\n",
    "\n",
    "for i in range(len(days)-1):\n",
    "\n",
    "    print(f'day is {days[i]}')\n",
    "    # For Republican subreddit\n",
    "    params = {\n",
    "        'subreddit': 'Republican',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params)\n",
    "    print(res.status_code)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        df_posts = pd.DataFrame(posts)\n",
    "        print(len(posts))\n",
    "        list_of_reps5.append(df_posts)\n",
    "    \n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # For democrats subreddit\n",
    "    params2 = {\n",
    "        'subreddit': 'democrats',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res2 = requests.get(url, params2)\n",
    "    print(res2.status_code)\n",
    "\n",
    "    if res2.status_code == 200:\n",
    "        data2 = res2.json()\n",
    "        posts2 = data2['data']\n",
    "        df_posts2 = pd.DataFrame(posts2)\n",
    "        print(len(posts2))\n",
    "        list_of_dems5.append(df_posts2)\n",
    "    \n",
    "    \n",
    "    #time between requests\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data for days in range 2240 to 3000 from today\n",
    "**downloads up to day 2741 were successful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "days = []\n",
    "for i in range(2240,3000):\n",
    "    days.append(str(i) + 'd')\n",
    "\n",
    "list_of_reps6 = []\n",
    "list_of_dems6 = []\n",
    "\n",
    "for i in range(len(days)-1):\n",
    "\n",
    "    print(f'day is {days[i]}')\n",
    "    # For Republican subreddit\n",
    "    params = {\n",
    "        'subreddit': 'Republican',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params)\n",
    "    print(res.status_code)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        df_posts = pd.DataFrame(posts)\n",
    "        print(len(posts))\n",
    "        list_of_reps6.append(df_posts)\n",
    "    \n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # For democrats subreddit\n",
    "    params2 = {\n",
    "        'subreddit': 'democrats',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res2 = requests.get(url, params2)\n",
    "    print(res2.status_code)\n",
    "\n",
    "    if res2.status_code == 200:\n",
    "        data2 = res2.json()\n",
    "        posts2 = data2['data']\n",
    "        df_posts2 = pd.DataFrame(posts2)\n",
    "        print(len(posts2))\n",
    "        list_of_dems6.append(df_posts2)\n",
    "    \n",
    "    \n",
    "    #time between requests\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data for days in range 2735 to 3000 from today\n",
    "**all downloads were successful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "for i in range(2735,3000):\n",
    "    days.append(str(i) + 'd')\n",
    "\n",
    "list_of_reps7 = []\n",
    "list_of_dems7 = []\n",
    "\n",
    "for i in range(len(days)-1):\n",
    "\n",
    "    print(f'day is {days[i]}')\n",
    "    # For Republican subreddit\n",
    "    params = {\n",
    "        'subreddit': 'Republican',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params)\n",
    "    print(res.status_code)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        df_posts = pd.DataFrame(posts)\n",
    "        print(len(posts))\n",
    "        list_of_reps7.append(df_posts)\n",
    "    \n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # For democrats subreddit\n",
    "    params2 = {\n",
    "        'subreddit': 'democrats',\n",
    "        'before':days[i],\n",
    "        'after':days[i+1],\n",
    "        'size': 100\n",
    "    }\n",
    "\n",
    "    res2 = requests.get(url, params2)\n",
    "    print(res2.status_code)\n",
    "\n",
    "    if res2.status_code == 200:\n",
    "        data2 = res2.json()\n",
    "        posts2 = data2['data']\n",
    "        df_posts2 = pd.DataFrame(posts2)\n",
    "        print(len(posts2))\n",
    "        list_of_dems7.append(df_posts2)\n",
    "    \n",
    "    \n",
    "    #time between requests\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert lists of data into dataframes and export for future analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems = pd.concat(list_of_dems)\n",
    "dems2 = pd.concat(list_of_dems2)\n",
    "dems3 = pd.concat(list_of_dems3)\n",
    "dems4 = pd.concat(list_of_dems4)\n",
    "dems5 = pd.concat(list_of_dems5)\n",
    "dems6 = pd.concat(list_of_dems6)\n",
    "dems7 = pd.concat(list_of_dems7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems.to_csv('../data/dems.csv', index=False)\n",
    "dems2.to_csv('../data/dems2.csv', index=False)\n",
    "dems3.to_csv('../data/dems3.csv', index=False)\n",
    "dems4.to_csv('../data/dems4.csv', index=False)\n",
    "dems5.to_csv('../data/dems5.csv', index=False)\n",
    "dems6.to_csv('../data/dems6.csv', index=False)\n",
    "dems7.to_csv('../data/dems7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = pd.concat(list_of_reps)\n",
    "reps2 = pd.concat(list_of_reps2)\n",
    "reps3 = pd.concat(list_of_reps3)\n",
    "reps4 = pd.concat(list_of_reps4)\n",
    "reps5 = pd.concat(list_of_reps5)\n",
    "reps6 = pd.concat(list_of_reps6)\n",
    "reps7 = pd.concat(list_of_reps7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps.to_csv('../data/reps.csv', index=False)\n",
    "reps2.to_csv('../data/reps2.csv', index=False)\n",
    "reps3.to_csv('../data/reps3.csv', index=False)\n",
    "reps4.to_csv('../data/reps4.csv', index=False)\n",
    "reps5.to_csv('../data/reps5.csv', index=False)\n",
    "reps6.to_csv('../data/reps6.csv', index=False)\n",
    "reps7.to_csv('../data/reps7.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
